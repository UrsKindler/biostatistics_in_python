## **12. Decision Trees & Neural Networks**

### Overview
- **Purpose**: Predict outcomes using supervised learning algorithms
- **Use Case**: Classification, regression, feature importance assessment
- **Prerequisites**: Labeled training data, train/test split
- **Data Types**: **Mixed data types** (numeric + categorical for trees), **Numeric** (neural networks)

### When to Use
#### Decision Criteria
- **Use Decision Trees when**: Interpretability needed, mixed data types
- **Use Neural Networks when**: Complex patterns, large datasets
- **Required**: Sufficient training data, proper validation
- **Consider**: Overfitting prevention, hyperparameter tuning

### Data Type Requirements

#### Decision Trees:
- **Numeric Features**: Integer, Float (continuous/discrete)
- **Categorical Features**: String, Object, Category (handles natively)
- **Ordinal Features**: Ordered categories
- **Target Variable**: Categorical (classification) or Numeric (regression)
- **Missing Values**: Can handle missing data
- **Mixed Data**: Excellent for heterogeneous datasets

#### Neural Networks:
- **Numeric Features**: Float, Double (required)
- **Encoded Categoricals**: One-hot encoded categorical variables
- **Standardized Data**: Features should be scaled
- **Target Variable**: Numeric (regression) or Encoded categorical (classification)
- **Complete Cases**: No missing values
- **Not Suitable For**: Raw categorical text without encoding

### R vs Python

| Aspect | R | Python |
|--------|---|--------|
| Package | `rpart`, `neuralnet` | `sklearn.tree`, `sklearn.neural_network` |
| Decision Tree | `rpart(target ~ ., data)` | `DecisionTreeClassifier()` |
| Neural Network | `neuralnet(target ~ ., data)` | `MLPClassifier()` |
| Cross-validation | `caret::train()` | `cross_val_score()` |
| Model Evaluation | `confusionMatrix()` | `classification_report()` |

### Quick Start Code

**R Example:**
```r
# Load required libraries
library(rpart)
library(rpart.plot)
library(caret)
library(neuralnet)

# Decision tree
mytree <- rpart(target ~ ., data = TrainingSet)
rpart.plot(mytree, extra = 4)

# Predictions
predictions <- predict(mytree, newdata = TestingSet, type = "class")
confusionMatrix(table(predictions, TestingSet$target))

# Neural network
nn_model <- neuralnet(target ~ ., data = training_data, 
                     hidden = c(5, 3), threshold = 0.01)
```

**Python Example:**
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler

# Prepare data (assuming last column is target)
X = df.iloc[:, :-1]  # Features
y = df.iloc[:, -1]   # Target

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Decision Tree
tree = DecisionTreeClassifier(random_state=42, max_depth=5)
tree.fit(X_train, y_train)
tree_pred = tree.predict(X_test)
tree_accuracy = accuracy_score(y_test, tree_pred)

print(f"Decision Tree Accuracy: {tree_accuracy:.3f}")
print("\nDecision Tree Report:")
print(classification_report(y_test, tree_pred))

# Neural Network (scale features first)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42)
mlp.fit(X_train_scaled, y_train)
mlp_pred = mlp.predict(X_test_scaled)
mlp_accuracy = accuracy_score(y_test, mlp_pred)

print(f"\nNeural Network Accuracy: {mlp_accuracy:.3f}")
print("\nNeural Network Report:")
print(classification_report(y_test, mlp_pred))

# Feature importance (Decision Tree only)
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': tree.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 5 Important Features:")
print(feature_importance.head())
```
