# Model Evaluation Metrics

### Overview
- **Purpose**: Comprehensively evaluate machine learning model performance
- **Use Case**: Model selection, performance reporting, deployment decision
- **Prerequisites**: Trained model with predictions on test/validation set
- **Data Types**: Labeled data for classification or regression

### When to Use
#### Decision Criteria
- **Use always**: After training any supervised learning model
- **Essential when**: Comparing multiple models, reporting results
- **Critical for**: Deployment decisions, model monitoring
- **Report multiple**: Never rely on single metric

#### Data Type Requirements
- **Classification**: Categorical outcomes (binary or multi-class)
- **Regression**: Continuous numeric targets
- **Sample Size**: Minimum 30 test samples for reliable metrics
- **Not Suitable For**: Unsupervised learning, unlabeled data

### R vs Python

| Aspect | R | Python |
|--------|---|--------|
| Package | `caret`, `MLmetrics`, `pROC` | `sklearn.metrics` |
| Confusion Matrix | `confusionMatrix()` | `confusion_matrix()` |
| ROC Curve | `pROC::roc()` | `roc_curve()` |
| AUC | `pROC::auc()` | `roc_auc_score()` |
| Classification Report | `caret::confusionMatrix()` | `classification_report()` |
| Regression Metrics | `postResample()` | `mean_squared_error()`, `r2_score()` |

### Quick Start Code

**R Example:**
```r
Load required libraries
library(caret)
library(pROC)

Classification metrics
cm <- confusionMatrix(predictions, actual)
print(cm)

ROC and AUC
roc_obj <- roc(actual, predicted_probabilities)
plot(roc_obj)
auc(roc_obj)

Regression metrics
postResample(pred = predictions, obs = actual)
```

**Python Example:**
```python
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
f1_score, confusion_matrix, classification_report,
roc_curve, roc_auc_score, mean_squared_error, r2_score)

Classification Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-Score: {f1:.3f}")

Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

ROC Curve and AUC (binary classification)
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
auc_score = roc_auc_score(y_test, y_prob)
print(f"\nROC-AUC: {auc_score:.3f}")

Regression Metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"MAE: {mae:.3f}")
print(f"RMSE: {rmse:.3f}")
print(f"RÂ²: {r2:.3f}")
```
