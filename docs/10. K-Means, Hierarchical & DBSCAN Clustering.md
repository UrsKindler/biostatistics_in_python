## **10. K-Means, Hierarchical & DBSCAN Clustering**

### Overview
- **Purpose**: Group similar observations into clusters without prior group knowledge
- **Use Case**: Pattern discovery, sample classification, data exploration
- **Prerequisites**: Numeric data, scaled features, distance measure selection
- **Data Types**: **Continuous numeric** (multivariate)

### When to Use
#### Decision Criteria
- **Use K-Means when**: Spherical clusters expected, know approximate cluster number
- **Use Hierarchical when**: Want dendrogram, don't know cluster number
- **Use DBSCAN when**: Arbitrary cluster shapes, noise/outliers present
- **Always**: Validate with silhouette analysis and multiple methods

### Data Type Requirements
- **Continuous Data**: Float, Double (all features)
- **Scaled Data**: Features on comparable scales (standardization recommended)
- **Multivariate**: Multiple numeric dimensions
- **Distance Calculable**: Euclidean, Manhattan distances applicable
- **No Missing Values**: Complete cases required
- **Not Suitable For**: Categorical variables (without encoding), text data, mixed data types

### R vs Python

| Aspect | R | Python |
|--------|---|--------|
| Package | `factoextra`, `NbClust` | `sklearn.cluster` |
| K-Means | `eclust(data, "kmeans", k=3)` | `KMeans(n_clusters=3)` |
| Hierarchical | `eclust(data, "hclust")` | `AgglomerativeClustering()` |
| DBSCAN | `fpc::dbscan(data, eps=0.5)` | `DBSCAN(eps=0.5)` |
| Optimal K | `NbClust()` | `silhouette_score()` |

### Quick Start Code

**R Example:**
```r
# Load required libraries
library(factoextra)
library(NbClust)
library(fpc)  # for DBSCAN

# K-means clustering
nb <- NbClust(matrix, distance = "euclidean", min.nc = 2, max.nc = 5, 
              method = "kmeans", index = "all")
k_means <- eclust(matrix, "kmeans", hc_metric = "euclidean", k = 3, nstart = 25)

# Hierarchical clustering  
h_clust <- eclust(matrix, "hclust", hc_metric = "euclidean", hc_method = "ward.D")

# DBSCAN
dbscan_result <- dbscan(matrix, eps = 0.5, MinPts = 5)
```

**Python Example:**
```python
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Standardize data first
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(df_scaled)
kmeans_silhouette = silhouette_score(df_scaled, kmeans_labels)

# Hierarchical clustering
hierarchical = AgglomerativeClustering(n_clusters=3)
hier_labels = hierarchical.fit_predict(df_scaled)
hier_silhouette = silhouette_score(df_scaled, hier_labels)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(df_scaled)
if len(set(dbscan_labels)) > 1:
    dbscan_silhouette = silhouette_score(df_scaled, dbscan_labels)
else:
    dbscan_silhouette = -1

# Compare methods
print(f"K-means silhouette: {kmeans_silhouette:.3f}")
print(f"Hierarchical silhouette: {hier_silhouette:.3f}")
print(f"DBSCAN silhouette: {dbscan_silhouette:.3f}")

# Find optimal number of clusters for K-means
silhouette_scores = []
for k in range(2, 11):
    kmeans_k = KMeans(n_clusters=k, random_state=42)
    labels_k = kmeans_k.fit_predict(df_scaled)
    silhouette_scores.append(silhouette_score(df_scaled, labels_k))

optimal_k = range(2, 11)[silhouette_scores.index(max(silhouette_scores))]
print(f"Optimal K: {optimal_k}")
```
