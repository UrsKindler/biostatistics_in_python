## **4. Normalization & Z-Score Normalization**

### Overview
- **Purpose**: Scale data to comparable ranges and normalize distributions
- **Use Case**: Before PCA, clustering, or when features have different scales
- **Prerequisites**: Numeric data, understanding of data distribution

### When to Use
#### Decision Criteria
- **Use Z-score when**: Features have different scales, before PCA/clustering
- **Use Min-Max when**: Need bounded values (0-1), neural networks
- **Use Log transform when**: Right-skewed data, fold-change data
- **Use Robust scaling when**: Outliers present

### R vs Python

| Aspect | R | Python |
|--------|---|--------|
| Package | `base R` | `sklearn.preprocessing`, `pandas` |
| Z-score | `scale(data)` | `StandardScaler()` |
| Min-Max | `(data - min) / (max - min)` | `MinMaxScaler()` |
| Log Transform | `log(data + 1)` | `pandas log functions` |
| Robust Scaling | Custom with median/MAD | `RobustScaler()` |

### Quick Start Code

**Python Simple:**
```python
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# Z-score normalization
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), 
                        columns=df.columns, index=df.index)

# Min-max normalization
minmax_scaler = MinMaxScaler()
df_minmax = pd.DataFrame(minmax_scaler.fit_transform(df), 
                        columns=df.columns, index=df.index)

# Log transformation using pandas
df_log = (df + 1).apply(lambda x: x.apply(lambda y: y.log() if y > 0 else 0))
# Or simpler:
df_log = df.apply(lambda x: (x + 1).apply('log'))

# Robust scaling (less sensitive to outliers)
robust_scaler = RobustScaler()
df_robust = pd.DataFrame(robust_scaler.fit_transform(df), 
                        columns=df.columns, index=df.index)
```
